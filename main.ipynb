{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7bfffe",
   "metadata": {},
   "source": [
    "# Single Human Target Locking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a6d2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-24 22:45:14.041 Python[7970:5117197] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/core/src/dxt.cpp:3508: error: (-215:Assertion failed) type == CV_32FC1 || type == CV_32FC2 || type == CV_64FC1 || type == CV_64FC2 in function 'dft'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[1;32m     21\u001b[0m bb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mselectROI(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/core/src/dxt.cpp:3508: error: (-215:Assertion failed) type == CV_32FC1 || type == CV_32FC2 || type == CV_64FC1 || type == CV_64FC2 in function 'dft'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "TrDict = {'csrt': cv2.TrackerCSRT_create,\n",
    "         'kcf' : cv2.TrackerKCF_create,\n",
    "         'boosting' : cv2.legacy.TrackerBoosting_create,\n",
    "         'mil': cv2.TrackerMIL_create,\n",
    "         'tld': cv2.legacy.TrackerTLD_create,\n",
    "         'medianflow': cv2.legacy.TrackerMedianFlow_create,\n",
    "         'mosse':cv2.legacy.TrackerMOSSE_create}\n",
    "\n",
    "tracker = TrDict['csrt']()\n",
    "\n",
    "v = cv2.VideoCapture('/Users/xy0ke/Downloads/multiple-object-tracking-in-videos-using-opencv-in-python-master/Youtube/Video 01/data/mot.mp4') # video\n",
    "# v = cv2.VideoCapture(0)\n",
    "v.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "v.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "ret, frame = v.read()\n",
    "cv2.imshow('Frame', frame)\n",
    "\n",
    "bb = cv2.selectROI('Frame', frame)\n",
    "tracker.init(frame, bb)\n",
    "\n",
    "while True:\n",
    "    ret, frame = v.read()\n",
    "    timer = cv2.getTickCount()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    (success,box) = tracker.update(frame)\n",
    "\n",
    "    if success:\n",
    "        (x,y,w,h) = [int(a) for a in box]\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(100,255,0),2)\n",
    "\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    cv2.putText(frame, \"FPS : \" + str(int(fps)), (100,50), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50), 2)\n",
    "\n",
    "    cv2.imshow('Frame',frame)\n",
    "\n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "v.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537dda0",
   "metadata": {},
   "source": [
    "# Multiple Human Target Locking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "import sys\n",
    "import cv2\n",
    "from random import randint\n",
    "\n",
    "\n",
    "trackerTypes = ['BOOSTING', 'MIL', 'KCF', 'TLD', 'MEDIANFLOW', 'GOTURN', 'MOSSE', 'CSRT']\n",
    "\n",
    "\n",
    "def createTrackerByName(trackerType):\n",
    "    # Create a tracker based on tracker name\n",
    "    if trackerType == trackerTypes[0]:\n",
    "        tracker = cv2.legacy.TrackerBoosting_create()\n",
    "    elif trackerType == trackerTypes[1]:\n",
    "        tracker = cv2.legacy.TrackerMIL_create()\n",
    "    elif trackerType == trackerTypes[2]:\n",
    "        tracker = cv2.legacy.TrackerKCF_create()\n",
    "    elif trackerType == trackerTypes[3]:\n",
    "        tracker = cv2.legacy.TrackerTLD_create()\n",
    "    elif trackerType == trackerTypes[4]:\n",
    "        tracker = cv2.legacy.TrackerMedianFlow_create()\n",
    "    elif trackerType == trackerTypes[5]:\n",
    "        tracker = cv2.legacy.TrackerGOTURN_create()\n",
    "    elif trackerType == trackerTypes[6]:\n",
    "        tracker = cv2.TrackerMOSSE_create()\n",
    "    elif trackerType == trackerTypes[7]:\n",
    "        tracker = cv2.legacy.TrackerCSRT_create()\n",
    "    else:\n",
    "        tracker = None\n",
    "        print(\"Incorrect tracker name\")\n",
    "        print(\"Available trackers are:\")\n",
    "        for t in trackerTypes:\n",
    "            print(t)\n",
    "\n",
    "    return tracker\n",
    "\n",
    "\n",
    "# Set video to load\n",
    "videoPath = \"/Users/xy0ke/Downloads/multiple-object-tracking-in-videos-using-opencv-in-python-master/Youtube/Video 01/data/mot.mp4\"\n",
    "\n",
    "# Create a video capture object to read videos\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "# Read first frame\n",
    "success, frame = cap.read()\n",
    "# quit if unable to read the video file\n",
    "if not success:\n",
    "    print(\"Failed to read video\")\n",
    "    sys.exit(1)\n",
    "\n",
    "    ## Select boxes\n",
    "bboxes = []\n",
    "colors = []\n",
    "\n",
    "# OpenCV's selectROI function doesn't work for selecting multiple objects in Python\n",
    "# So we will call this function in a loop till we are done selecting all objects\n",
    "while True:\n",
    "    # draw bounding boxes over objects\n",
    "    # selectROI's default behaviour is to draw box starting from the center\n",
    "    # when fromCenter is set to false, you can draw box starting from top left corner\n",
    "    bbox = cv2.selectROI(\"MultiTracker\", frame)\n",
    "    bboxes.append(bbox)\n",
    "    colors.append((randint(0, 255), randint(0, 255), randint(0, 255)))\n",
    "    print(\"Press q to quit selecting boxes and start tracking\")\n",
    "    print(\"Press any other key to select next object\")\n",
    "    k = cv2.waitKey(0) & 0xFF\n",
    "    print(k)\n",
    "    if k == 113:  # q is pressed\n",
    "        break\n",
    "\n",
    "print(\"Selected bounding boxes {}\".format(bboxes))\n",
    "\n",
    "# Specify the tracker type\n",
    "trackerType = \"CSRT\"\n",
    "createTrackerByName(trackerType)\n",
    "\n",
    "# Create MultiTracker object\n",
    "multiTracker = cv2.legacy.MultiTracker_create()\n",
    "\n",
    "\n",
    "# Initialize MultiTracker\n",
    "for bbox in bboxes:\n",
    "    multiTracker.add(createTrackerByName(trackerType), frame, bbox)\n",
    "\n",
    "    # Process video and track objects\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    timer = cv2.getTickCount()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # get updated location of objects in subsequent frames\n",
    "    success, boxes = multiTracker.update(frame)\n",
    "\n",
    "    # draw tracked objects\n",
    "    for i, newbox in enumerate(boxes):\n",
    "        p1 = (int(newbox[0]), int(newbox[1]))\n",
    "        p2 = (int(newbox[0] + newbox[2]), int(newbox[1] + newbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, colors[i], 2, 1)\n",
    "\n",
    "    # show frame\n",
    "    fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        \"FPS : \" + str(int(fps)),\n",
    "        (100, 50),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.75,\n",
    "        (50, 170, 50),\n",
    "        2,\n",
    "    )\n",
    "    cv2.imshow(\"MultiTracker\", frame)\n",
    "\n",
    "    # quit on ESC button\n",
    "    if cv2.waitKey(1) & 0xFF == 27:  # Esc pressed\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860adb47",
   "metadata": {},
   "source": [
    "# Multiple Human Target No Locking (YOLO v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "\n",
    "frame_width, frame_height = [1280, 720]\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n",
    "\n",
    "model = YOLO(\"/Users/xy0ke/Desktop/YOLO v8 on mac m1/yolov8n.pt\")\n",
    "\n",
    "box_annotator = sv.BoxAnnotator(\n",
    "    thickness=2,\n",
    "    text_thickness=2,\n",
    "    text_scale=1\n",
    ")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    result = model(frame, agnostic_nms=True, classes=[0])[0]\n",
    "    detections = sv.Detections.from_yolov8(result)\n",
    "    \n",
    "    labels = [\n",
    "        f\"{model.model.names[class_id]} {round(confidence*100, 2)}%\"\n",
    "        for _, _, confidence, class_id, _ in detections\n",
    "        \n",
    "    ]\n",
    "    frame = box_annotator.annotate(\n",
    "        scene=frame, \n",
    "        detections=detections,\n",
    "        labels=labels\n",
    "    )     \n",
    "    \n",
    "    cv2.imshow(\"yolov8\", frame)\n",
    "\n",
    "    if (cv2.waitKey(30) == 27):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee6c93",
   "metadata": {},
   "source": [
    "# Human Targets with Trailing System No Locking (YOLO v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e717015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 124.9ms\n",
      "Speed: 5.4ms preprocess, 124.9ms inference, 14.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "2023-09-24 15:49:41.020 Python[4623:5014284] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n",
      "\n",
      "0: 384x640 1 person, 188.9ms\n",
      "Speed: 4.0ms preprocess, 188.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Get the boxes and track IDs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m boxes \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxywh\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m---> 29\u001b[0m track_ids \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Visualize the results on the frame\u001b[39;00m\n\u001b[1;32m     32\u001b[0m annotated_frame \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'int'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Open the video file\n",
    "# video_path = \"/Users/xy0ke/Desktop/YOLO v8 on mac m1/car.mp4\"\n",
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture('/Users/xy0ke/Downloads/multiple-object-tracking-in-videos-using-opencv-in-python-master/Youtube/Video 01/data/mot.mp4') # video\n",
    "\n",
    "# Store the track history\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(source=frame, persist=True, classes=[0])\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        boxes = results[0].boxes.xywh.cpu()\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Plot the tracks\n",
    "        for box, track_id in zip(boxes, track_ids):\n",
    "            x, y, w, h = box\n",
    "            track = track_history[track_id]\n",
    "            track.append((float(x), float(y)))  # x, y center point\n",
    "            if len(track) > 30:  # retain 90 tracks for 90 frames\n",
    "                track.pop(0)\n",
    "\n",
    "            # Draw the tracking lines\n",
    "            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if (cv2.waitKey(30) == 27):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
